---
title: "mb6-power-analysis"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(broom)
library(lme4)
library(broom.mixed)
theme_set(theme_bw())
```


# Notes from reading MS on analytic design

w/i ss (tongue protusion & mouth opening) each of tongue and mouth has 3x (gesture + passive), but I think we're going over the whole time period (so 2 trials/infant, 2-3 measures per trial)

* number of tongue protrusions 
* number of mouth open 
* time w/ mouth open 

count data is likely to be positive skewed & zero inflated

multilevel regression to account for "design factors" as well as chi-squre goodness of fit and Wilcoxon matched-paired signed-rank tests

random intercepts for infant & lab (nested, but will do separate if there are convergence issues) (VB note: nested here is just notational -- the nesting is a property of the data not of the model; the nesting notation only matters if you didn't uniqueify your subject labels between labs)

"systematically add to base model one random slope fx per model" (VB: this seems hella weird as an approach -- they should pick the maximal relevant thing and stick with it. Also probably go Bayesian for backup that won't have convergence issues.)

VB note: shouldn't infant age go into the models somehow, especially given the 2 age category design??

fixed fx: adult_demonstration , infant_response and their interaction 

^ this only works for the frequency based ones! like this has to be count/count (but elsewhere it said that time was also going to be used for mouth open)

effect coding (+1 tongue, -1 mouth) (VB thinks +.5/ -.5 would be better for magnitude interpretation )

statistical assumptions of distribution -- count data is poisson or neg binomial (and perhaps zero inflated)
paper suggests data transformations and z-scores (VB thinks is bad idea)

planned comparisons about DV ~ adult_behavior 
for DV -- # mouth, # tongue, amount of mouth 

chi-sq count data on more or less in each table; there's argument over what the null is because there's 0/0 stuff. 

wilcoxon matched-pairs signed-rank test
One Wilcoxon test will assess whether infants produce significantly more tongue protrusion responses to the adult tongue-protrusion display than to the adult mouth-opening display; and a second Wilcoxon test will assess whether infants produce significantly more mouth opening responses to the adult mouth-opening display than to the adult tongue-protrusion display. 

if we do neg binomial would we log-link?

# Notes from MS on  power analysis
goal is 95% power at alpha=.05

1000 reps each 

simulating from a poisson:
* expected effect size (d={.2, .3, .4}) (had citations)
* intra-correlations between infant tongue and infant mouth opening responses (.2 or .4) (had citations)
* lab to lab variation (SD = .25) <- where does this come from 
  * 6 or 10 labs
  * assume lab sample with N=32 for one age 
  * but there are also plans to do potential "half-sample" as well. 
  
Models:
* count data (adult_beh x infant_beh)
* count data tongue (adult_beh)
* count data mouth (adult_beh)
* time data mouth (adult_beh)
* chi-square test (???)
* wilcoxon test

# simulating data

Questions -- what should be the functional form of the distribution being simulated (poisson, neg binomial, zero-inflated?)

poisson -- 1 parameter model (mean=var) <-- seems not ideal when 
neg binomial -- 2 parameter model, mu=(r(1-p)/p), p (same as poisson for p=1) (parameterizations vary)

I think neg binomial for sampling gives the most flexibility! to have mean and variance be different, should be able to accommodate things. 


## how do mean and sd map to params for neg binom?

mean = n(1-p)/p
var = n(1-p)/p**2

going to use the mu and size/dispersion param:
var = mu + mu**2/disp

```{r}
sample <- expand_grid(mean=c(3), dispersion=c(.5, 1,5, 10,100), d=c(.2, .3, .4)) |> 
  mutate(dat = pmap(list(mean, dispersion, d), \(m,disp,d) {
     var=m + m**2/disp
     sd=sqrt(var)
    delta=.5*d*sqrt(var)
    m_lower=m-delta
    dat_1=rnbinom(10000, size=disp, mu=m_lower)
    m_upper=m+delta
    dat_2=rnbinom(10000, size=disp, mu=m_upper)
    tibble(dat=dat_1, type="lower") |> bind_rows(tibble(dat=dat_2, type="upper")) |> mutate(sd=sd)
  })) |> unnest(dat)
```


### what do different dispersions look like?

```{r}
sample |> 
  ggplot(aes(x=dat, fill=type))+geom_histogram(binwidth = 1,alpha=.5, position=position_identity())+facet_grid(dispersion~d) +coord_cartesian(xlim=c(0,20))
```


### are we accurate about the cohen's d?
```{r}

check <- sample |> group_by(type, dispersion, d, sd) |> summarize(m_calc=mean(dat), sd_calc=sd(dat)) |> 
  pivot_wider(names_from=type, values_from=c(m_calc, sd_calc)) |> 
  mutate(pooled_sd=sqrt((sd_calc_lower**2+sd_calc_upper**2)/2),
         d_calc=(m_calc_upper-m_calc_lower)/pooled_sd)

check |> ggplot(aes(x=d, y=d_calc, color=as.character(dispersion)))+geom_point()+geom_abline()

```

note: this is assuming independence (which is how these were sampled), but doesn't take into account covariance!

# What would I do?

* overall mean (3ish)
* dispersion (guess/ try a range of reasonable / pilot) (size parameter) (higher = more poisson-like)
* d is determined by mean and (pooled) variance
* lab sampling value
* individual sampling value (this is to get the correlations / cov to work out)

value = sample from neg_binom with mean = mean + effect + lab_var + indiv_var 

so, given that the individual sampling, lab sampling, and d are all in units of sd (or could be), may be add those up and then do? 

```{r}
sample_neg_binom=function(effect, mean, dispersion){
  var=mean + mean**2/dispersion
  delta=effect*sqrt(var)
  new_mean=mean+delta
  rnbinom(n=1, size=dispersion, mu=new_mean)
  
}
```

for different values of dispersion (keeping mean=3), what effect would get different correlations. 

```{r}

test <- expand_grid(child=c(1:30), dispersion=c(1,5,10,100), effect=c(.1, .2, .3, .4, .5), rep=1:100) |> 
  group_by(dispersion, effect, rep) |> 
  nest() |> 
  mutate(result=map(data, \(d){
    sample <- d |> 
      rowwise() |> 
    mutate(child_delta=rnorm(n=1, mean=0, sd=effect),
           sample_1=sample_neg_binom(child_delta, mean=3, dispersion=dispersion),
           sample_2=sample_neg_binom(child_delta, mean=3, dispersion=dispersion))
    cor.test(sample$sample_1, sample$sample_2, method="spearman")$est
  })) |> 
  select(-data) |> 
  unnest(result)


test |> ggplot(aes(x=effect, y=result))+geom_point(alpha=.1)+stat_summary()+geom_hline(yintercept=.2, lty="dashed")+facet_wrap(~dispersion)

```
so if we think the correlation between two of the counts should get a spearman's rho of .2 then we think the sd of the kiddo distribution should be like .4-.5 (in standardized units)

and if we think the by-site variation is like .25 standardized units, we can start figuring this out...

```{r}
library(lme4)
library(broom.mixed)

test <- expand_grid(sites=c(10), #how many sites (this is the "power" part)
                    dispersion=c(10), # we don't know what dispersion will be, so try something for now -- will want to try other options for some robustness
                    effect_size=c(.2), # in standardized units, what is match vs mismatch effect (interaction)
                    site_sd=.25, #in standardized units, how variable are sites
                    child_sd=.5, #in standardized units, how variable are child (correlation across 4 measures, but expressed this way)
                    rep=1:2 #increase later for testing
                    ) |> 
  group_by(dispersion, effect_size, rep, sites) |> #we want a power for each of these groups
  nest() |> 
  mutate(result=map(data, \(d){
    sim<- d |> expand_grid(site_id=1:sites) |> rowwise() |> #generate simulated sites
      mutate(site_delta=rnorm(n=1, mean=0, sd=site_sd)) |> 
      expand_grid(participant_id=c(1:32)) |> #generate simulated kiddos
      #what is actual kid/site / age?? counts??
      rowwise() |> 
    mutate(child_delta=rnorm(n=1, mean=0, sd=child_sd)) |> 
      expand_grid(adult_did=c("A","B"), child_did=c("A","B")) |> #generate simulated conditions
      rowwise() |> 
      mutate(effect=ifelse(adult_did==child_did, effect_size/2, -effect_size/2),
           sample=sample_neg_binom(child_delta+site_delta+effect, mean=3, dispersion=dispersion))
    
        glmer.nb(sample~adult_did*child_did+(1|site_id/participant_id), data=sim) |> tidy()
        # age???
        # should figure out what we actually want and pre-pull
        # do we want more ranef here??? note that we aren't building in any true slopes?
  })) |> 
  select(-data) |> 
  unnest(result)


test |> filter(effect=="fixed")
```

# Questions for Mike

* age -- how is age getting incorporated?
 options:
  * separate models for younger & older (potential interpretation/multiple test issues)
  * as interaction effect in one model (adult_beh*infant_beh*age)

* so, what's the model we want to use (at least for power analysis) (age, and random slopes)


* distributional assumptions -- the more (apparently) zero inflated neg-binomials have some really long tails. do we want to windsorize to something reasonable (max observed in 90 seconds? 15 if 3 is avg?)

* parameter selections: we need to choose values (or ranges) to test

(distributional)
- (grand) mean: 3 (from supposed prior work, should also check robustness after)
- dispersion: ??? (definitely check multiple values here)
- max value (mostly matters for low values on dispersion parameter): 

(fixed effects)
* interaction (match/mismatch) effect (in units of cohen's d): .2, .3, .4
* simple effect (A > B): 0 (but should def do robustness checks with other numbers on some range)
* effect of age (and interaction with age)

(random effects)
* sd of lab distribution (in units of cohen's d): .25 (????)
* sd of infant effects (from correlation of some infants just do more things, but in units of cohen's d): ???
* lab or infant specific slopes ??? <-- idk if we want to include these or not

